{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzTK7gM5ydFL"
      },
      "source": [
        "# **IMPORT LIBS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG6VE-z-iIFN"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout,Conv1D,Flatten\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "#from skimage.transform import rotate, shear, zoom\n",
        "#from imgaug import augmenters as iaa\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make auxiliar folders\n",
        "if not os.path.exists('runtime_saves'):\n",
        "    os.makedirs('runtime_saves')\n",
        "if not os.path.exists('runtime_saves/models'):\n",
        "    os.makedirs('runtime_saves/models')\n",
        "if not os.path.exists('runtime_saves/train&test'):\n",
        "    os.makedirs('runtime_saves/train&test')\n",
        "    \n",
        "current_dir = os.getcwd()\n",
        "\n",
        "root_dir = os.path.abspath(os.path.join(current_dir, os.pardir, os.pardir, os.pardir))\n",
        "\n",
        "datasetUAH_dir = os.path.join(root_dir, 'datasets', 'UAH-DRIVESET-v1', 'UAH-Processed')\n",
        "\n",
        "print(f'Root directory: {root_dir}')\n",
        "print(f'Dataset directory: {datasetUAH_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHBmH_88Ilpx"
      },
      "source": [
        "# **AUX FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0lXdqGfInKu"
      },
      "outputs": [],
      "source": [
        "def save_manovers_positions_to_csv_file(gps_positions, manovers, filename):\n",
        "  output = np.zeros_like(gps_positions)\n",
        "\n",
        "  # Iterate through the elements of arr2\n",
        "  for i in range(len(manovers)):\n",
        "    # Check if the element in arr2 is 1\n",
        "    if manovers[i] == 1:\n",
        "      # Copy the corresponding values from arr1 to the output array\n",
        "      output[i] = gps_positions[i]\n",
        "\n",
        "  output = output[~np.all(output == 0, axis=1)]\n",
        "  \n",
        "  filename = 'runtime_saves/' + filename\n",
        "    \n",
        "  np.savetxt(filename, output, delimiter=',', fmt='%.9f')\n",
        "\n",
        "\n",
        "\n",
        "def separate_positives_negatives(data):\n",
        "  # Ensure the input is converted to a NumPy array for easier manipulation\n",
        "  data = np.array(data)\n",
        "\n",
        "  # Create two empty arrays to store positive and negative values\n",
        "  positives = np.zeros_like(data)\n",
        "  negatives = np.zeros_like(data)\n",
        "\n",
        "  # Use boolean indexing to separate positive and negative values\n",
        "  positives[data > 0] = data[data > 0]\n",
        "  negatives[data < 0] = -data[data < 0]\n",
        "\n",
        "  # Combine the positive and negative values into a single 2D array\n",
        "  return (positives, negatives)\n",
        "\n",
        "def normalize_between_0_and_max(data):\n",
        "  max_value = np.max(data)\n",
        "  return data / max_value\n",
        "\n",
        "def normalize_between_0_and_max_v2(data, max_value):\n",
        "  return data / max_value\n",
        "\n",
        "def split_train_test(data, test_size=0.2):\n",
        "  # Check if test_size is between 0 and 1\n",
        "  if test_size < 0 or test_size > 1:\n",
        "    raise ValueError(\"test_size must be between 0 and 1.\")\n",
        "\n",
        "  # Get the number of samples\n",
        "  num_samples = data.shape[0]\n",
        "\n",
        "  # Calculate the number of samples for each set\n",
        "  train_size = int(num_samples * (1 - test_size))\n",
        "  test_size = num_samples - train_size\n",
        "\n",
        "  # Randomly shuffle the data for better splitting (optional)\n",
        "  #np.random.shuffle(data)\n",
        "\n",
        "  # Split the data into training and test sets\n",
        "  train_data = data[:train_size]\n",
        "  test_data = data[train_size:]\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "def y_classification(data, threshold):\n",
        "  classification = np.zeros_like(data, dtype=int)  # Initialize output array\n",
        "\n",
        "  for col in range(0, 12):  # Loop through each column\n",
        "    max_value = np.max(data[:, col])\n",
        "    threshold_pos = max_value * threshold\n",
        "    classification[:, col] = np.where(data[:, col] >= threshold_pos, 1, 0)\n",
        "\n",
        "  return classification\n",
        "\n",
        "def max_of_vectors(vec1, vec2, vec3, vec4, vec5, vec6):\n",
        "  # Combine all vectors into a single array\n",
        "  all_vectors = np.array([vec1, vec2, vec3, vec4, vec5, vec6])\n",
        "\n",
        "  # Find the maximum value in the array\n",
        "  max_value = np.max(all_vectors)\n",
        "\n",
        "  return max_value\n",
        "\n",
        "def has_one(data):\n",
        "  \"\"\"\n",
        "  This function receives a numpy array and returns a new array\n",
        "  with 1 if the correspondent row of input array has at least one cellule with 1.\n",
        "  In other case the cellule is 0.\n",
        "\n",
        "  Args:\n",
        "      data: A numpy array of shape (n, 12) with 0 or 1 values in each cell.\n",
        "\n",
        "  Returns:\n",
        "      A numpy array of shape (n, 1) with 1s where the corresponding row in data has at least one 1, and 0s otherwise.\n",
        "  \"\"\"\n",
        "  # We sum each row, and any value greater than zero indicates at least one 1 in that row\n",
        "  return np.sum(data, axis=1)[:, np.newaxis] > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aATQYo4TiWrU"
      },
      "source": [
        "# **IMPORT DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5wPPy3OiYj9"
      },
      "outputs": [],
      "source": [
        "# Empty dataframe necessary columns\n",
        "df = pd.DataFrame(columns=['accelerometerXAxis', 'accelerometerYAxis', 'accelerometerZAxis', 'gyroscopeXAxis', 'gyroscopeYAxis', 'gyroscopeZAxis', 'latitude', 'longitude'])\n",
        "\n",
        "\n",
        "acelX = df['accelerometerXAxis']\n",
        "acelY = df['accelerometerYAxis']\n",
        "acelZ = df['accelerometerZAxis']\n",
        "\n",
        "gyrX = df['gyroscopeXAxis']\n",
        "gyrY = df['gyroscopeYAxis']\n",
        "gyrZ = df['gyroscopeZAxis']\n",
        "\n",
        "latitude = df['latitude']\n",
        "longitude = df['longitude']\n",
        "\n",
        "# Loop through all the files in the dataset directory\n",
        "for root, dirs, files in os.walk(datasetUAH_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):\n",
        "            df_aux = pd.read_csv(os.path.join(root, file))\n",
        "            \n",
        "            acelX = pd.concat([acelX, df_aux['accelerometerXAxis']])\n",
        "            acelY = pd.concat([acelY, df_aux['accelerometerYAxis']])\n",
        "            acelZ = pd.concat([acelZ, df_aux['accelerometerZAxis']])\n",
        "            gyrX = pd.concat([gyrX, df_aux['gyroscopeXAxis']])\n",
        "            gyrY = pd.concat([gyrY, df_aux['gyroscopeYAxis']])\n",
        "            gyrZ = pd.concat([gyrZ, df_aux['gyroscopeZAxis']])\n",
        "            latitude = pd.concat([latitude, df_aux['latitude']])\n",
        "            longitude = pd.concat([longitude, df_aux['longitude']])\n",
        "            \n",
        "            print(f'{file} added to the dataframe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzW3XSugY8m3",
        "outputId": "809859f8-a9ef-4638-c9be-f3536c2556c9"
      },
      "outputs": [],
      "source": [
        "print(df['accelerometerXAxis'].describe())\n",
        "print(df['accelerometerYAxis'].describe())\n",
        "print(df['accelerometerZAxis'].describe())\n",
        "print(df['gyroscopeXAxis'].describe())\n",
        "print(df['gyroscopeYAxis'].describe())\n",
        "print(df['gyroscopeZAxis'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb3I99C7ASnM"
      },
      "source": [
        "# **SEPARATE DATA BY MANOVER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaJIdH4qvx0t"
      },
      "outputs": [],
      "source": [
        "turnRightX, turnLeftX = separate_positives_negatives(acelX)\n",
        "\n",
        "accelY, breakY = separate_positives_negatives(acelY)\n",
        "\n",
        "positiveZ, negativeZ = separate_positives_negatives(acelZ)\n",
        "\n",
        "gyrPositiveX, gyrNegativeX = separate_positives_negatives(gyrX)\n",
        "gyrPositiveY, gyrNegativeY = separate_positives_negatives(gyrY)\n",
        "gyrPositiveZ, gyrNegativeZ = separate_positives_negatives(gyrZ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHxP7SVfGrX6",
        "outputId": "462b33e6-f35f-4fe1-bc33-f89dcfe106b5"
      },
      "outputs": [],
      "source": [
        "turnRightX.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N01_nop2Znt5"
      },
      "source": [
        "# **CLASSIFICATION BASED ON NOT NORMALIZED VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELrWyhmuZnHS"
      },
      "outputs": [],
      "source": [
        "#x1 = np.array(list(zip(turnRightX, turnLeftX, accelY, breakY, positiveZ, negativeZ, gyrPositiveX, gyrNegativeX, gyrPositiveY, gyrNegativeY, gyrPositiveZ, gyrNegativeZ)))\n",
        "\n",
        "#y = y_classification(x1, 0.25)\n",
        "#print (np.sum(y, axis=0))\n",
        "#np.savetxt(\"Y.csv\", y, delimiter=',', fmt='%.0i')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPIA30JzCcST"
      },
      "source": [
        "# **NORMALIZE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTCRAhTXZXNL"
      },
      "outputs": [],
      "source": [
        "max_accel = max_of_vectors(turnRightX, turnLeftX, accelY, breakY, positiveZ, negativeZ)\n",
        "max_gyr = max_of_vectors(gyrPositiveX, gyrNegativeX, gyrPositiveY, gyrNegativeY, gyrPositiveZ, gyrNegativeZ)\n",
        "\n",
        "turnRightXn = normalize_between_0_and_max_v2(turnRightX, max_accel)\n",
        "turnLeftXn = normalize_between_0_and_max_v2(turnLeftX, max_accel)\n",
        "accelYn = normalize_between_0_and_max_v2(accelY, max_accel)\n",
        "breakYn = normalize_between_0_and_max_v2(breakY, max_accel)\n",
        "positiveZn = normalize_between_0_and_max_v2(positiveZ, max_accel)\n",
        "negativeZn = normalize_between_0_and_max_v2(negativeZ, max_accel)\n",
        "gyrPositiveXn = normalize_between_0_and_max_v2(gyrPositiveX, max_gyr)\n",
        "gyrNegativeXn = normalize_between_0_and_max_v2(gyrNegativeX, max_gyr)\n",
        "gyrPositiveYn = normalize_between_0_and_max_v2(gyrPositiveY, max_gyr)\n",
        "gyrNegativeYn = normalize_between_0_and_max_v2(gyrNegativeY, max_gyr)\n",
        "gyrPositiveZn = normalize_between_0_and_max_v2(gyrPositiveZ, max_gyr)\n",
        "gyrNegativeZn = normalize_between_0_and_max_v2(gyrNegativeZ, max_gyr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrtvVRZ0Avz8"
      },
      "source": [
        "# **CREATE AN ARRAY WITH ALL DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppkRUVMX1c7W"
      },
      "outputs": [],
      "source": [
        "x = np.array(list(zip(turnRightXn, turnLeftXn, accelYn, breakYn, positiveZn, negativeZn, gyrPositiveXn, gyrNegativeXn, gyrPositiveYn, gyrNegativeYn, gyrPositiveZn, gyrNegativeZn)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX4JSKAddzy0"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJVtNOyF3bU5",
        "outputId": "a8fca71e-9321-4b0d-d334-51b8ba23f123"
      },
      "outputs": [],
      "source": [
        "y = y_classification(x, 0.3)\n",
        "print (np.sum(y, axis=0))\n",
        "\n",
        "filename = 'runtime_saves/' + 'Y.csv'\n",
        "print(y)\n",
        "\n",
        "np.savetxt(filename, y, delimiter=',', fmt='%.0i')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IZvV0MhdLBy"
      },
      "source": [
        "# **SHOW MANOVERS ON GOOLGE MAP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvW2QnkTdRiA"
      },
      "outputs": [],
      "source": [
        "positions = np.array(list(zip(latitude, longitude)))\n",
        "manover_accelY = y[:, 2]\n",
        "manover_breakY = y[:, 3]\n",
        "manover_turnRightXn = y[:, 0]\n",
        "manover_turnLeftXn = y[:, 1]\n",
        "gyrPositiveZn = y[:, 10]\n",
        "gyrNegativeZn = y[:, 11]\n",
        "save_manovers_positions_to_csv_file(positions, manover_accelY, \"accelY.csv\")\n",
        "save_manovers_positions_to_csv_file(positions, manover_breakY, \"breakY.csv\")\n",
        "save_manovers_positions_to_csv_file(positions, manover_turnRightXn, \"turnRightX.csv\")\n",
        "save_manovers_positions_to_csv_file(positions, manover_turnLeftXn, \"turnLeftX.csv\")\n",
        "save_manovers_positions_to_csv_file(positions, gyrPositiveZn, \"gyrPositZ.csv\")\n",
        "save_manovers_positions_to_csv_file(positions, gyrNegativeZn, \"gyrNegZ.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_78VHvBRJQ"
      },
      "source": [
        "# **PLOT MANOVERS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cksFepBiioYu"
      },
      "outputs": [],
      "source": [
        "plt.plot(y[:, 2], marker='.', linestyle='none')\n",
        "plt.plot(y[:, 3], marker='.', linestyle='none')\n",
        "\n",
        "plt.legend(['Accel Y', 'Break Y'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB-CdF_MGToJ"
      },
      "source": [
        "# **SEPARATE DATA IN TRAIN AND TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd56BA3fIORH"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = split_train_test(x, test_size=0.2)\n",
        "\n",
        "y_train, y_test = split_train_test(y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d5xKgrEC-ks"
      },
      "source": [
        "# **CREATE THE INPUT TENSORES DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOkN2T88doeE"
      },
      "outputs": [],
      "source": [
        "train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
        "test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "np.savetxt(\"runtime_saves/train&test/x_train.csv\", x_train, delimiter=',', fmt='%.9f')\n",
        "np.savetxt(\"runtime_saves/train&test/x_test.csv\", x_test, delimiter=',', fmt='%.9f')\n",
        "np.savetxt(\"runtime_saves/train&test/y_train.csv\", y_train, delimiter=',', fmt='%.0i')\n",
        "np.savetxt(\"runtime_saves/train&test/y_test.csv\", y_test, delimiter=',', fmt='%.0i')\n",
        "\n",
        "np.savetxt(\"runtime_saves/train&test/train.csv\", train.reshape(train.shape[0], train.shape[2]), delimiter=',', fmt='%.9f')\n",
        "np.savetxt(\"runtime_saves/train&test/test.csv\", test.reshape(test.shape[0], test.shape[2]), delimiter=',', fmt='%.9f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBfgiQgAGZPW"
      },
      "source": [
        "# **CREATE THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wdxfd2JJwmh"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "\n",
        "dropout = 0.01\n",
        "# learning_rate = 0.001\n",
        "\n",
        "#conv1D and LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(1, 12)))\n",
        "model_lstm.add(LSTM(128, return_sequences=True))\n",
        "model_lstm.add(Dropout(dropout))\n",
        "model_lstm.add(LSTM(64, return_sequences=False))\n",
        "model_lstm.add(Dropout(dropout))\n",
        "model_lstm.add(Dense(12, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khawMeLUGczr"
      },
      "source": [
        "# **TRAIN THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNkg4Mr-K36t",
        "outputId": "c5ced2dc-cc07-4cc5-d922-f176e6e16542"
      },
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, mode='min', min_lr=0.0000)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model_lstm_output = model_lstm.fit(train, y_train, epochs=10, batch_size=16, validation_split=0.2, shuffle=True, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct_S460uGhxP"
      },
      "source": [
        "# **SHOW THE RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLflDJutrQ0h"
      },
      "outputs": [],
      "source": [
        "plt.plot(model_lstm_output.history['loss'])\n",
        "plt.plot(model_lstm_output.history['val_loss'])\n",
        "plt.title('Historico de train')\n",
        "plt.xlabel('Epocas de train')\n",
        "plt.ylabel('Função custo')\n",
        "plt.legend(['Erro train', 'Erro test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTtErVAPKNQT"
      },
      "outputs": [],
      "source": [
        "accuracy = model_lstm.evaluate(test, y_test)[1]  # Assuming accuracy is the second metric\n",
        "print('Test Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5yRYVkqpv51"
      },
      "source": [
        "# **TEST THE NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTRP1HLnH5VA",
        "outputId": "16236fd5-cb67-4cc2-8018-fe7b9b86945e"
      },
      "outputs": [],
      "source": [
        "loss, accurary = model_lstm.evaluate(test, y_test, batch_size=16)\n",
        "print('Test loss/accurary:', loss, accurary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_classes = np.argmax(y_train, axis=1)\n",
        "\n",
        "y_pred = model_lstm.predict(train)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_train_classes, y_pred_classes)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()\n",
        "\n",
        "print(model_lstm_output.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLTGBDbnb5kg"
      },
      "outputs": [],
      "source": [
        "test[0]\n",
        "test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCXg55GxwbGg"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "for i in range(100):\n",
        "    a = x_test[i]\n",
        "    b = a.reshape(1, 1, 12)\n",
        "\n",
        "    # Make predictions on new data\n",
        "    prediction = model_lstm.predict(b)\n",
        "    #predicted_class = label_encoder.inverse_transform(prediction)[0]\n",
        "\n",
        "    np.round(prediction, decimals=1, out=prediction)\n",
        "    np.round(x_test[i], decimals=1, out=x_test[i])\n",
        "    #print(\"Value:\", newArray[i + start])\n",
        "    if (np.sum(y_test[i]) > 0):\n",
        "      print(\"X [:\", x_test[i])\n",
        "      print(\"Y [:\", y_test[i])\n",
        "      print(\"PC:\", prediction)\n",
        "      print (i)\n",
        "    i = 1 + 1\n",
        "#PREDICTIONS WITH COLAB MODEL\n",
        "#prediction = model.predict(teste)\n",
        "#print(\"Predicted class:\", prediction)\n",
        "#print(\"Predicted class:\", predicted_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmrX4Kzdc0T3",
        "outputId": "3c468fe8-622f-476e-f75c-750a4878ea26"
      },
      "outputs": [],
      "source": [
        "test_value = np.array([0., 0.363, 0.313, 0., 0., 0.31, 0.393, 0., 0., 0.244, 0.247, 0.])\n",
        "test_value = test_value.reshape(1, 1, 12)\n",
        "\n",
        "# Make predictions on new data\n",
        "prediction = model_lstm.predict(test_value)\n",
        "np.round(prediction, decimals=2, out=prediction)\n",
        "\n",
        "print(\"Value    :\", test_value[0][0])\n",
        "print(\"Predicted:\", prediction[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02BdeK0gJrOU"
      },
      "source": [
        "# **SAVE THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "KGOyK8rXJvmx",
        "outputId": "7d0a5249-f640-4945-b90c-feaa6b10b3ee"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "model_name = 'lstm_model_CONV' + datetime.datetime.now().strftime(\"%Y-%m-%d %HH%Mm%Ss\") + '.h5'\n",
        "\n",
        "# Save the model in runtime_saves/models folder\n",
        "model_lstm.save(os.path.join(\".\", 'runtime_saves', 'models', model_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model sumary\n",
        "model_lstm.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
