{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzTK7gM5ydFL"
      },
      "source": [
        "# **IMPORT LIBS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG6VE-z-iIFN"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "#from skimage.transform import rotate, shear, zoom\n",
        "#from imgaug import augmenters as iaa\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make auxiliar folders\n",
        "if not os.path.exists('runtime_saves'):\n",
        "    os.makedirs('runtime_saves')\n",
        "if not os.path.exists('runtime_saves/models'):\n",
        "    os.makedirs('runtime_saves/models')\n",
        "if not os.path.exists('runtime_saves/train&test'):\n",
        "    os.makedirs('runtime_saves/train&test')\n",
        "    \n",
        "current_dir = os.getcwd()\n",
        "\n",
        "root_dir = os.path.abspath(os.path.join(current_dir, os.pardir, os.pardir, os.pardir))\n",
        "\n",
        "datasetUAH_dir = os.path.join(root_dir, 'datasets', 'UAH-DRIVESET-v1', 'UAH-Processed')\n",
        "\n",
        "print(f'Root directory: {root_dir}')\n",
        "print(f'Dataset directory: {datasetUAH_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHBmH_88Ilpx"
      },
      "source": [
        "# **AUX FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0lXdqGfInKu"
      },
      "outputs": [],
      "source": [
        "def save_manovers_positions_to_csv_file(gps_positions, manovers, filename):\n",
        "  output = np.zeros_like(gps_positions)\n",
        "\n",
        "  # Iterate through the elements of arr2\n",
        "  for i in range(len(manovers)):\n",
        "    # Check if the element in arr2 is 1\n",
        "    if manovers[i] == 1:\n",
        "      # Copy the corresponding values from arr1 to the output array\n",
        "      output[i] = gps_positions[i]\n",
        "\n",
        "  output = output[~np.all(output == 0, axis=1)]\n",
        "  \n",
        "  filename = 'runtime_saves/' + filename\n",
        "    \n",
        "  np.savetxt(filename, output, delimiter=',', fmt='%.9f')\n",
        "\n",
        "\n",
        "\n",
        "def separate_positives_negatives(data):\n",
        "  # Ensure the input is converted to a NumPy array for easier manipulation\n",
        "  data = np.array(data)\n",
        "\n",
        "  # Create two empty arrays to store positive and negative values\n",
        "  positives = np.zeros_like(data)\n",
        "  negatives = np.zeros_like(data)\n",
        "\n",
        "  # Use boolean indexing to separate positive and negative values\n",
        "  positives[data > 0] = data[data > 0]\n",
        "  negatives[data < 0] = -data[data < 0]\n",
        "\n",
        "  # Combine the positive and negative values into a single 2D array\n",
        "  return (positives, negatives)\n",
        "\n",
        "def normalize_between_0_and_max(data):\n",
        "  max_value = np.max(data)\n",
        "  return data / max_value\n",
        "\n",
        "def normalize_between_0_and_max_v2(data, max_value):\n",
        "  return data / max_value\n",
        "\n",
        "def split_train_test(data, test_size=0.2):\n",
        "  # Check if test_size is between 0 and 1\n",
        "  if test_size < 0 or test_size > 1:\n",
        "    raise ValueError(\"test_size must be between 0 and 1.\")\n",
        "\n",
        "  # Get the number of samples\n",
        "  num_samples = data.shape[0]\n",
        "\n",
        "  # Calculate the number of samples for each set\n",
        "  train_size = int(num_samples * (1 - test_size))\n",
        "  test_size = num_samples - train_size\n",
        "\n",
        "  # Randomly shuffle the data for better splitting (optional)\n",
        "  #np.random.shuffle(data)\n",
        "\n",
        "  # Split the data into training and test sets\n",
        "  train_data = data[:train_size]\n",
        "  test_data = data[train_size:]\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "def y_classification(data, threshold):\n",
        "  classification = np.zeros_like(data, dtype=int)  # Initialize output array\n",
        "\n",
        "  for col in range(0, 12):  # Loop through each column\n",
        "    max_value = np.max(data[:, col])\n",
        "    threshold_pos = max_value * threshold\n",
        "    classification[:, col] = np.where(data[:, col] >= threshold_pos, 1, 0)\n",
        "\n",
        "  return classification\n",
        "\n",
        "def max_of_vectors(vec1, vec2, vec3, vec4, vec5, vec6):\n",
        "  # Combine all vectors into a single array\n",
        "  all_vectors = np.array([vec1, vec2, vec3, vec4, vec5, vec6])\n",
        "\n",
        "  # Find the maximum value in the array\n",
        "  max_value = np.max(all_vectors)\n",
        "\n",
        "  return max_value\n",
        "\n",
        "def has_one(data):\n",
        "  \"\"\"\n",
        "  This function receives a numpy array and returns a new array\n",
        "  with 1 if the correspondent row of input array has at least one cellule with 1.\n",
        "  In other case the cellule is 0.\n",
        "\n",
        "  Args:\n",
        "      data: A numpy array of shape (n, 12) with 0 or 1 values in each cell.\n",
        "\n",
        "  Returns:\n",
        "      A numpy array of shape (n, 1) with 1s where the corresponding row in data has at least one 1, and 0s otherwise.\n",
        "  \"\"\"\n",
        "  # We sum each row, and any value greater than zero indicates at least one 1 in that row\n",
        "  return np.sum(data, axis=1)[:, np.newaxis] > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aATQYo4TiWrU"
      },
      "source": [
        "# **IMPORT DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5wPPy3OiYj9"
      },
      "outputs": [],
      "source": [
        "dataset = os.path.join(os.getcwd(), os.pardir, 'docs', 'v2', 'dataset-all.csv')\n",
        "# dataset = os.path.join(os.getcwd(), os.pardir, 'docs', 'v1', 'Abrantes-Leiria.csv')\n",
        "\n",
        "\n",
        "df = pd.read_csv(dataset)\n",
        "\n",
        "\n",
        "acelX = df['accelerometerXAxis']\n",
        "acelY = df['accelerometerYAxis']\n",
        "acelZ = df['accelerometerZAxis']\n",
        "\n",
        "gyrX = df['gyroscopeXAxis']\n",
        "gyrY = df['gyroscopeYAxis']\n",
        "gyrZ = df['gyroscopeZAxis']\n",
        "\n",
        "latitude = df['latitude']\n",
        "longitude = df['longitude']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzW3XSugY8m3",
        "outputId": "809859f8-a9ef-4638-c9be-f3536c2556c9"
      },
      "outputs": [],
      "source": [
        "print(df['accelerometerXAxis'].describe())\n",
        "print(df['accelerometerYAxis'].describe())\n",
        "print(df['accelerometerZAxis'].describe())\n",
        "print(df['gyroscopeXAxis'].describe())\n",
        "print(df['gyroscopeYAxis'].describe())\n",
        "print(df['gyroscopeZAxis'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb3I99C7ASnM"
      },
      "source": [
        "# **SEPARATE DATA BY MANOVER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaJIdH4qvx0t"
      },
      "outputs": [],
      "source": [
        "turnRightX, turnLeftX = separate_positives_negatives(acelX)\n",
        "\n",
        "accelY, breakY = separate_positives_negatives(acelY)\n",
        "\n",
        "positiveZ, negativeZ = separate_positives_negatives(acelZ)\n",
        "\n",
        "gyrPositiveX, gyrNegativeX = separate_positives_negatives(gyrX)\n",
        "gyrPositiveY, gyrNegativeY = separate_positives_negatives(gyrY)\n",
        "gyrPositiveZ, gyrNegativeZ = separate_positives_negatives(gyrZ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHxP7SVfGrX6",
        "outputId": "462b33e6-f35f-4fe1-bc33-f89dcfe106b5"
      },
      "outputs": [],
      "source": [
        "turnRightX.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPIA30JzCcST"
      },
      "source": [
        "# **NORMALIZE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTCRAhTXZXNL"
      },
      "outputs": [],
      "source": [
        "max_accel = max_of_vectors(turnRightX, turnLeftX, accelY, breakY, positiveZ, negativeZ)\n",
        "max_gyr = max_of_vectors(gyrPositiveX, gyrNegativeX, gyrPositiveY, gyrNegativeY, gyrPositiveZ, gyrNegativeZ)\n",
        "\n",
        "turnRightXn = normalize_between_0_and_max_v2(turnRightX, max_accel)\n",
        "turnLeftXn = normalize_between_0_and_max_v2(turnLeftX, max_accel)\n",
        "accelYn = normalize_between_0_and_max_v2(accelY, max_accel)\n",
        "breakYn = normalize_between_0_and_max_v2(breakY, max_accel)\n",
        "positiveZn = normalize_between_0_and_max_v2(positiveZ, max_accel)\n",
        "negativeZn = normalize_between_0_and_max_v2(negativeZ, max_accel)\n",
        "gyrPositiveXn = normalize_between_0_and_max_v2(gyrPositiveX, max_gyr)\n",
        "gyrNegativeXn = normalize_between_0_and_max_v2(gyrNegativeX, max_gyr)\n",
        "gyrPositiveYn = normalize_between_0_and_max_v2(gyrPositiveY, max_gyr)\n",
        "gyrNegativeYn = normalize_between_0_and_max_v2(gyrNegativeY, max_gyr)\n",
        "gyrPositiveZn = normalize_between_0_and_max_v2(gyrPositiveZ, max_gyr)\n",
        "gyrNegativeZn = normalize_between_0_and_max_v2(gyrNegativeZ, max_gyr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrtvVRZ0Avz8"
      },
      "source": [
        "# **CREATE AN ARRAY WITH ALL DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppkRUVMX1c7W"
      },
      "outputs": [],
      "source": [
        "x = np.array(list(zip(turnRightXn, turnLeftXn, accelYn, breakYn, positiveZn, negativeZn, gyrPositiveXn, gyrNegativeXn, gyrPositiveYn, gyrNegativeYn, gyrPositiveZn, gyrNegativeZn)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX4JSKAddzy0"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJVtNOyF3bU5",
        "outputId": "a8fca71e-9321-4b0d-d334-51b8ba23f123"
      },
      "outputs": [],
      "source": [
        "y = y_classification(x, 0.3)\n",
        "print (np.sum(y, axis=0))\n",
        "\n",
        "filename = 'runtime_saves/' + 'Y.csv'\n",
        "print(y)\n",
        "\n",
        "np.savetxt(filename, y, delimiter=',', fmt='%.0i')\n",
        "\n",
        "# y classification based on outliers\n",
        "#TODO: implement y classification based on outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "\n",
        "# Load the dataset\n",
        "dataset = os.path.join(os.getcwd(), os.pardir, 'docs', 'v2', 'dataset-all.csv')\n",
        "data = pd.read_csv(dataset)\n",
        "\n",
        "columns = ['accelerometerXAxis', 'accelerometerYAxis', 'accelerometerZAxis', 'gyroscopeXAxis', 'gyroscopeYAxis', 'gyroscopeZAxis']\n",
        "\n",
        "data = data[columns]\n",
        "\n",
        "#convert all columns to float\n",
        "data = data.astype(float)\n",
        "\n",
        "\n",
        "# Detect and visualize outliers for each sensor column using IQR method\n",
        "for column in data.columns:\n",
        "    #print max value in column\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 6 * IQR\n",
        "    upper_bound = Q3 + 6 * IQR\n",
        "\n",
        "    # Create a mask for outliers\n",
        "    mask = (data[column] >= lower_bound) & (data[column] <= upper_bound)\n",
        "\n",
        "    # Plot the results\n",
        "    # fig, ax = plt.subplots(figsize=(16, 6))\n",
        "    # fig.suptitle(f'IQR-Based Outlier Detection for {column}', size=20)\n",
        "\n",
        "    # # Plot without needing a timestamp\n",
        "    # sns.scatterplot(data=data, x=np.arange(len(data)), y=column, hue=np.where(mask, 'No Outlier', 'Outlier'), ax=ax)\n",
        "    \n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    # print outlier values\n",
        "    print(f'Outliers for {column}:')\n",
        "    #how many of ofliers in that column\n",
        "    print(data[~mask][column].shape)\n",
        "    print(data[~mask][column])\n",
        "    \n",
        "#create new data with only outliers\n",
        "outliers = data[~mask]\n",
        "\n",
        "agressive_values_positive = {}\n",
        "agressive_values_negative = {}\n",
        "\n",
        "for column in outliers.columns:\n",
        "    print(f\"Max value for {column}: {data[column].max()} and Min value for {column}: {data[column].min()}\")\n",
        "    \n",
        "    top_positive_values = data[data[column] > 0][column].nlargest(outliers[column].shape[0])\n",
        "    top_negative_values = data[data[column] < 0][column].nsmallest(outliers[column].shape[0])\n",
        "    \n",
        "    agressive_values_positive[column] = top_positive_values.mean()\n",
        "    agressive_values_negative[column] = top_negative_values.mean()\n",
        "print()\n",
        "# Print the representative values for each column\n",
        "print(\"Agressive values for top positive values:\")\n",
        "for column, value in agressive_values_positive.items():\n",
        "    print(f\"Value for {column}: {value}\")\n",
        "print()\n",
        "print(\"Agressive values for top negative values:\")\n",
        "for column, value in agressive_values_negative.items():\n",
        "    print(f\"Value for {column}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "dataset = os.path.join(os.getcwd(), os.pardir, 'docs', 'v2', 'dataset-all.csv')\n",
        "data = pd.read_csv(dataset)\n",
        "\n",
        "columns = ['accelerometerXAxis', 'accelerometerYAxis', 'accelerometerZAxis', 'gyroscopeXAxis', 'gyroscopeYAxis', 'gyroscopeZAxis']\n",
        "\n",
        "data = data[columns]\n",
        "\n",
        "# Convert all columns to float\n",
        "data = data.astype(float)\n",
        "\n",
        "# Detect and visualize outliers for each sensor column using IQR method\n",
        "for column in data.columns:\n",
        "    # Calculate IQR\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for extreme outliers based on previous outliers\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Create a mask for outliers\n",
        "    mask = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
        "\n",
        "    # Further filter extreme outliers using the median\n",
        "    median = data[column].median()\n",
        "    mask_extreme_outliers = abs(data[column] - median) > 15 * IQR  # Adjust 3 as needed for your data\n",
        "\n",
        "    # Combine the masks to identify extreme outliers\n",
        "    mask_extreme = mask & mask_extreme_outliers\n",
        "\n",
        "    # # Plot the results\n",
        "    # fig, ax = plt.subplots(figsize=(16, 6))\n",
        "    # fig.suptitle(f'Extreme Outlier Detection for {column}', size=20)\n",
        "\n",
        "    # # Plot without needing a timestamp\n",
        "    # sns.scatterplot(data=data, x=np.arange(len(data)), y=column, hue=np.where(mask_extreme, 'Extreme Outlier', 'Normal'), ax=ax)\n",
        "    \n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    # Print extreme outlier values\n",
        "    print(f'Extreme Outliers for {column}:')\n",
        "    print(data[mask][column])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "dataset = os.path.join(os.getcwd(), os.pardir, 'docs', 'v2', 'dataset-all.csv')\n",
        "data = pd.read_csv(dataset)\n",
        "\n",
        "columns = ['accelerometerXAxis', 'accelerometerYAxis', 'accelerometerZAxis', 'gyroscopeXAxis', 'gyroscopeYAxis', 'gyroscopeZAxis']\n",
        "\n",
        "data = data[columns]\n",
        "\n",
        "# Convert all columns to float\n",
        "data = data.astype(float)\n",
        "\n",
        "# Define the number of top values to consider\n",
        "top_n = 1000\n",
        "\n",
        "# Calculate the representative values for each column\n",
        "agressive_values_positive = {}\n",
        "agressive_values_negative = {}\n",
        "for column in data.columns:\n",
        "    print(f\"Max value for {column}: {data[column].max()} and Min value for {column}: {data[column].min()}\")\n",
        "    \n",
        "    top_positive_values = data[data[column] > 0][column].nlargest(top_n)\n",
        "    top_negative_values = data[data[column] < 0][column].nsmallest(top_n)\n",
        "    \n",
        "    agressive_values_positive[column] = top_positive_values.mean()\n",
        "    agressive_values_negative[column] = top_negative_values.mean()\n",
        "print()\n",
        "# Print the representative values for each column\n",
        "print(\"Agressive values for top positive values:\")\n",
        "for column, value in agressive_values_positive.items():\n",
        "    print(f\"Value for {column}: {value}\")\n",
        "print()\n",
        "print(\"Agressive values for top negative values:\")\n",
        "for column, value in agressive_values_negative.items():\n",
        "    print(f\"Value for {column}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "dataset = os.path.join(os.getcwd(), os.pardir, 'docs', 'v2', 'dataset-all.csv')\n",
        "data = pd.read_csv(dataset)\n",
        "\n",
        "columns = ['accelerometerXAxis', 'accelerometerYAxis', 'accelerometerZAxis', 'gyroscopeXAxis', 'gyroscopeYAxis', 'gyroscopeZAxis']\n",
        "\n",
        "data = data[columns]\n",
        "\n",
        "# Convert all columns to float\n",
        "data = data.astype(float)\n",
        "\n",
        "# Define the number of top values to consider\n",
        "top_n = 5\n",
        "\n",
        "# Calculate statistics based on the top N values\n",
        "top_values = data.stack().nlargest(top_n)\n",
        "top_stats = {\n",
        "    'mean': top_values.mean(),\n",
        "    'std': top_values.std()\n",
        "}\n",
        "\n",
        "# Detect and visualize outliers for each sensor column using top N values statistics\n",
        "for column in data.columns:\n",
        "    # Calculate statistics for the column\n",
        "    column_stats = {\n",
        "        'mean': data[column].mean(),\n",
        "        'std': data[column].std()\n",
        "    }\n",
        "\n",
        "    # Detect outliers based on the top N values statistics\n",
        "    lower_bound = top_stats['mean'] - 3 * top_stats['std']  # Adjust the multiplier as needed\n",
        "    upper_bound = top_stats['mean'] + 3 * top_stats['std']  # Adjust the multiplier as needed\n",
        "    mask = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
        "\n",
        "    # Plot the results\n",
        "    fig, ax = plt.subplots(figsize=(16, 6))\n",
        "    fig.suptitle(f'Outlier Detection for {column}', size=20)\n",
        "\n",
        "    # Plot without needing a timestamp\n",
        "    sns.scatterplot(data=data, x=np.arange(len(data)), y=column, hue=np.where(mask, 'Outlier', 'Normal'), ax=ax)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print outlier values\n",
        "    print(f'Outliers for {column}:')\n",
        "    print(data[mask][column])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB-CdF_MGToJ"
      },
      "source": [
        "# **SEPARATE DATA IN TRAIN AND TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create sequences\n",
        "# if there is a label as agressive in the sequence, the sequence is labeled as agressive\n",
        "# if there is a label as normal in the sequence, the sequence is labeled as normal\n",
        "# y is the 2D array with the labels\n",
        "\n",
        "def create_sequences(data, y, sequence_length, step_size=1):\n",
        "    # Initialize the output arrays\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    # Iterate through the data to create sequences\n",
        "    for i in range(0, len(data) - sequence_length + 1, step_size):\n",
        "        # Define the start and end indices for the current sequence\n",
        "        start_index = i\n",
        "        end_index = i + sequence_length\n",
        "\n",
        "        # Extract the data and labels for the current sequence\n",
        "        sequence = data[start_index:end_index]\n",
        "        \n",
        "        # check by columns in the sequence, if there is a label as agressive, that column is labeled as agressive in the sequence\n",
        "        # For each column in the sequence, check if there is a label as agressive in that column. IF so, that column in the label array is labeled as agressive\n",
        "        \n",
        "        # Initialize the label for the current sequence\n",
        "        label = np.zeros(y.shape[1])\n",
        "        \n",
        "        # Iterate through the columns of the sequence\n",
        "        for j in range(y.shape[1]):\n",
        "            # Check if the label is agressive in the current column\n",
        "            if 1 in y[start_index:end_index, j]:\n",
        "                # Set the label as agressive for the current column\n",
        "                label[j] = 1\n",
        "                \n",
        "        # Append the sequence and label to the output arrays\n",
        "        X.append(sequence)\n",
        "        Y.append(label)\n",
        "        \n",
        "    return np.array(X), np.array(Y) \n",
        "\n",
        "xs, ys = x, y\n",
        "\n",
        "x, y = create_sequences(xs, ys, 40, 4)\n",
        "\n",
        "# Split the data into training and test sets with sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd56BA3fIORH"
      },
      "outputs": [],
      "source": [
        "# x_train, x_test = split_train_test(x, test_size=0.2)\n",
        "\n",
        "# y_train, y_test = split_train_test(y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d5xKgrEC-ks"
      },
      "source": [
        "# **CREATE THE INPUT TENSORES DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOkN2T88doeE"
      },
      "outputs": [],
      "source": [
        "train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
        "test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "np.savetxt(\"runtime_saves/train&test/x_train.csv\", x_train, delimiter=',', fmt='%.9f')\n",
        "np.savetxt(\"runtime_saves/train&test/x_test.csv\", x_test, delimiter=',', fmt='%.9f')\n",
        "np.savetxt(\"runtime_saves/train&test/y_train.csv\", y_train, delimiter=',', fmt='%.0i')\n",
        "np.savetxt(\"runtime_saves/train&test/y_test.csv\", y_test, delimiter=',', fmt='%.0i')\n",
        "\n",
        "np.savetxt(\"runtime_saves/train&test/train.csv\", train.reshape(train.shape[0], train.shape[2]), delimiter=',', fmt='%.9f')\n",
        "np.savetxt(\"runtime_saves/train&test/test.csv\", test.reshape(test.shape[0], test.shape[2]), delimiter=',', fmt='%.9f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBfgiQgAGZPW"
      },
      "source": [
        "# **CREATE THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wdxfd2JJwmh"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "\n",
        "dropout_l0 = 0.2\n",
        "dropout_l1 = 0.2\n",
        "# learning_rate = 0.001\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(50, input_shape=(1, train.shape[2]), return_sequences=True))\n",
        "model_lstm.add(Dropout(dropout_l0))\n",
        "model_lstm.add(LSTM(50, return_sequences=False))\n",
        "model_lstm.add(Dropout(dropout_l1))\n",
        "model_lstm.add(Dense(12,activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khawMeLUGczr"
      },
      "source": [
        "# **TRAIN THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNkg4Mr-K36t",
        "outputId": "c5ced2dc-cc07-4cc5-d922-f176e6e16542"
      },
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "model_lstm_output = model_lstm.fit(train, y_train, epochs=30, batch_size=32, validation_split=0.2, shuffle=True, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct_S460uGhxP"
      },
      "source": [
        "# **SHOW THE RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLflDJutrQ0h"
      },
      "outputs": [],
      "source": [
        "plt.plot(model_lstm_output.history['loss'])\n",
        "plt.plot(model_lstm_output.history['val_loss'])\n",
        "plt.title('Historico de train')\n",
        "plt.xlabel('Epocas de train')\n",
        "plt.ylabel('Função custo')\n",
        "plt.legend(['Erro train', 'Erro test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTtErVAPKNQT"
      },
      "outputs": [],
      "source": [
        "accuracy = model_lstm.evaluate(test, y_test)[1]  # Assuming accuracy is the second metric\n",
        "print('Test Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5yRYVkqpv51"
      },
      "source": [
        "# **TEST THE NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTRP1HLnH5VA",
        "outputId": "16236fd5-cb67-4cc2-8018-fe7b9b86945e"
      },
      "outputs": [],
      "source": [
        "loss, accurary = model_lstm.evaluate(test, y_test, batch_size=16)\n",
        "print('Test loss/accurary:', loss, accurary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# y_pred = model_lstm.predict(treino)\n",
        "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# # Plot confusion matrix\n",
        "# cm = confusion_matrix(y_treino, y_pred_classes)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "# disp.plot(cmap=plt.cm.Blues)\n",
        "# plt.title('Matriz de Confusão')\n",
        "# plt.show()\n",
        "\n",
        "# # Additional debugging: Print training history\n",
        "# print(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLTGBDbnb5kg"
      },
      "outputs": [],
      "source": [
        "test[0]\n",
        "test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCXg55GxwbGg"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "for i in range(100):\n",
        "    a = x_test[i]\n",
        "    b = a.reshape(1, 1, 12)\n",
        "\n",
        "    # Make predictions on new data\n",
        "    prediction = model_lstm.predict(b)\n",
        "    #predicted_class = label_encoder.inverse_transform(prediction)[0]\n",
        "\n",
        "    np.round(prediction, decimals=1, out=prediction)\n",
        "    np.round(x_test[i], decimals=1, out=x_test[i])\n",
        "    #print(\"Value:\", newArray[i + start])\n",
        "    if (np.sum(y_test[i]) > 0):\n",
        "      print(\"X [:\", x_test[i])\n",
        "      print(\"Y [:\", y_test[i])\n",
        "      print(\"PC:\", prediction)\n",
        "      print (i)\n",
        "    i = 1 + 1\n",
        "#PREDICTIONS WITH COLAB MODEL\n",
        "#prediction = model.predict(teste)\n",
        "#print(\"Predicted class:\", prediction)\n",
        "#print(\"Predicted class:\", predicted_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmrX4Kzdc0T3",
        "outputId": "3c468fe8-622f-476e-f75c-750a4878ea26"
      },
      "outputs": [],
      "source": [
        "test_value = np.array([0., 0.363, 0.313, 0., 0., 0.31, 0.393, 0., 0., 0.244, 0.247, 0.])\n",
        "test_value = test_value.reshape(1, 1, 12)\n",
        "\n",
        "# Make predictions on new data\n",
        "prediction = model_lstm.predict(test_value)\n",
        "np.round(prediction, decimals=2, out=prediction)\n",
        "\n",
        "print(\"Value    :\", test_value[0][0])\n",
        "print(\"Predicted:\", prediction[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02BdeK0gJrOU"
      },
      "source": [
        "# **SAVE THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "KGOyK8rXJvmx",
        "outputId": "7d0a5249-f640-4945-b90c-feaa6b10b3ee"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "model_name = 'lstm_model_' + datetime.datetime.now().strftime(\"%Y-%m-%d %HH%Mm%Ss\") + '.h5'\n",
        "\n",
        "# Save the model in runtime_saves/models folder\n",
        "model_lstm.save(os.path.join(\".\", 'runtime_saves', 'models', model_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model sumary\n",
        "model_lstm.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
